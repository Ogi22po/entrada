#########################
#    IN/OUT location    #
#########################

entrada.location.work=/Users/maartenwullink/sidn/development/tmp/entrada
#entrada.location.input=s3://sidn-entrada-test/pcap/
entrada.location.input=/Users/maartenwullink/sidn/development/tmp/pcap
#entrada.location.output=/Users/maartenwullink/sidn/development/tmp/pcap/parquet
entrada.location.output=s3://sidn-entrada-test2/database/
entrada.location.archive=/Users/maartenwullink/sidn/development/tmp/pcap/parquet/archive
#entrada.location.archive=s3://sidn-entrada-test/archive


###################
#    Entrada     #
###################
#engine can be hadoop or aws
entrada.engine=aws

#name of the entrada database and tables that should be created 
entrada.database.name=entrada
entrada.database.table.dns=dns
entrada.database.table.icmp=icmp

#skip the last pcap file in the input dir, this file might still be
#written to if rsync is used.
entrada.input.file.skipfirst=false
#execute the pcap processing very minute unless already running
entrada.execution.delay=60
#entrada.serverlist=ns1.dns.nl_ams1-ns1
entrada.serverlist=ns1.dns.nl_ams1-ns1
#max number of rows per parquet output file
entrada.parquet.max=3000000
#minutes before dns cache entries timeout
#required to match dns requests spanning multiple pcap files
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout=10
#minutes before tcp flows cache entries timeout
#expired tcp flows are discarded
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.tcp.flows=1
#minutes before fragmented ip cache entries timeout
#expired fragments are discarded
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.ip.fragmented=1
#pcap reader buffer in bytes, should be multiple of 4096 one of common disk sector sizes,
#or if other disk sector size is used then use this multiplier.
entrada.buffer.pcap.reader=65536

#enable parsing of ICMP packets
entrada.icmp.enable=true

#Mode can be any of: archive, delete, none
entrada.pcap.archive.mode=archive


###################
#    Database    #
###################

spring.datasource.jdbcUrl=jdbc:h2:file:${entrada.location.work}/entrada.db
spring.datasource.driverClassName=org.h2.Driver
spring.datasource.username=sa
spring.datasource.password=entrada
spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
spring.h2.console.enabled=true
spring.h2.console.path=/h2
spring.jpa.hibernate.ddl-auto=none
#spring.jpa.show-sql=true
#logging.level.org.hibernate.SQL=DEBUG

#flyway
spring.flyway.baseline-on-migrate=true
spring.flyway.validate-on-migrate=false
logging.level.org.flywaydb=INFO

spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false


###################
#    Resolvers    #
###################

# used to retrieve subnets of Google Public Resolver. dig TXT locations.publicdns.goog.
google.resolver.hostname=locations.publicdns.goog.
google.resolver.timeout=15

# where to retrieve subnets of OpenDNS resolver
opendns.resolver.url=https://www.opendns.com/network-map-data
opendns.resolver.timeout=15

# where to retrieve subnets of CloudFlare resolver
cloudflare.resolver.ipv4.url=https://www.cloudflare.com/ips-v4
cloudflare.resolver.ipv6.url=https://www.cloudflare.com/ips-v6
cloudflare.resolver.timeout=15

# quad9 subnets are read from resources file (no way to fetch externally)

#################
#  AWS config   #
#################

#bucket that should be used by entrada
cloud.aws.bucket=sidn-entrada-test2

# Needs to be false when not running on EC2
cloud.aws.stack.auto=false
cloud.aws.region.static=eu-west-1
cloud.aws.credentials.useDefaultAwsCredentialsChain=true

#minimum size of each file part
cloud.aws.upload.multipart.mb.size=5
#number of thredas to use when uploading to AWS
cloud.aws.upload.parallelism=10
#storage class fo parquet files uploaded to S3 bucket
cloud.aws.upload.upload.storage-class=STANDARD_IA
cloud.aws.upload.archive.storage-class=STANDARD_IA


#################
# Hadoop/Impala #
#################



##########
# Athena #
##########

athena.workgroup=primary
athena.driver.name=com.simba.athena.jdbc.Driver
athena.url=jdbc:awsathena://AwsRegion=${cloud.aws.region.static};
# where Athena will store the results of queries
athena.output.location=s3://${cloud.aws.bucket}/entrada-athena-output/
athena.log.level=4
athena.log.path=${entrada.location.work}/athena-logs/


###############
#   MaxMind   #
###############
#names of the database files as found in the maxmind download archive
geoip.maxmind.db.name.country=GeoLite2-Country.mmdb
geoip.maxmind.db.name.asn=GeoLite2-ASN.mmdb
#url to where to download the maxmind database archives
geoip.maxmind.db.url.country=https://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz
geoip.maxmind.db.url.asn=https://geolite.maxmind.com/download/geoip/database/GeoLite2-ASN.tar.gz

# where to find (or store) the MaxMind files
geoip.maxmind.location=${entrada.location.work}/maxmind
#max number of days to use db before new db version should be downloaded
geoip.maxmind.age.max=30


###############
#   Metrics   #
###############

graphite.prefix=test.entrada
graphite.host=entrada-mon.sidnlabs.nl
graphite.port=2003
#connect timeout in seconds
graphite.connect.timeout=3
#retention agregation, default 30sec
#http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf
graphite.retention=10
graphite.threshhold=200


###############
#   LOGGING   #
###############

logging.level.nl.sidnlabs=INFO

