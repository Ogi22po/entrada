#########################
#    Data Locations     #
#########################

#Set the work, input, output, archive directories

#work (always use local disk)
entrada.location.work=/Users/maartenwullink/sidn/development/tmp/entrada/work

#Local
#entrada.location.input=/Users/maartenwullink/sidn/development/tmp/entrada/pcap
#entrada.location.output=/Users/maartenwullink/sidn/development/tmp/entrada/database
#entrada.location.archive=/Users/maartenwullink/sidn/development/tmp/entrada/archive

#Hadoop HDFS
#entrada.location.input=${hdfs.nameservice}/user/entrada/pcap
#entrada.location.output=${hdfs.nameservice}/user/entrada/database
#entrada.location.archive=${hdfs.nameservice}/user/entrada/archive

#Amazon S3
entrada.location.input=s3://${aws.bucket}/input/
entrada.location.output=s3://${aws.bucket}/database/
entrada.location.archive=s3://${aws.bucket}/archive


###################
#    Entrada     #
###################
#engine can be local, hadoop or aws
#entrada.location.output must match the engine
#e.g.  engine == hadoop && entrada.location.output == hdfs://...
entrada.engine=aws

#name of the entrada database and tables that should be created 
entrada.database.name=entrada
entrada.database.table.dns=dns
entrada.database.table.icmp=icmp

#skip the last pcap file in the input dir, this file might still be
#written to if rsync is used.
entrada.input.file.skipfirst=false
#execute the pcap processing very minute unless already running
entrada.execution.delay=60
entrada.serverlist=ns1.dns.nl_anycast1,ns2.dns.nl_anycast2
#entrada.serverlist=
#max number of rows per parquet output file
entrada.parquet.max=3000000
#minutes before dns cache entries timeout
#required to match dns requests spanning multiple pcap files
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout=10
#minutes before tcp flows cache entries timeout
#expired tcp flows are discarded
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.tcp.flows=1
#minutes before fragmented ip cache entries timeout
#expired fragments are discarded
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.ip.fragmented=1
#pcap inpustream buffer in kilobytes
entrada.inputstream.buffer=64

#enable parsing of ICMP packets
entrada.icmp.enable=true

#Mode can be any of: archive, delete, none
entrada.pcap.archive.mode=archive

#check every x minutes if there are partitions that need to be compacted
entrada.parquet.compaction.interval=1
#the number of minutes to wait after the last rows have been written to the partition
#before compaction of table partition is performed.
entrada.parquet.compaction.age=1


###################
#    Database    #
###################

#Choose between PostgreSQL or H2 database

#PostgreSQL
#################################
spring.datasource.url=jdbc:postgresql://localhost:5432/entrada
spring.datasource.username=${pgsql_entrada_user}
spring.datasource.password=${pgsql_entrada_password}
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
spring.datasource.driver-class-name=org.postgresql.Driver
#################################

#H2
#################################
#spring.datasource.url=jdbc:h2:file:${entrada.location.work}/entrada.db
#spring.datasource.driver-class-name=org.h2.Driver
#spring.datasource.username=sa
#spring.datasource.password=entrada
#spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
#spring.h2.console.enabled=true
#spring.h2.console.path=/h2
#spring.jpa.hibernate.ddl-auto=none
##################################

#JPA options
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
spring.jpa.generate-ddl=false
spring.jpa.properties.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.hbm2ddl.auto=none
spring.jpa.show-sql=false
spring.jpa.hibernate.format_sql=false
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false

#Connectionpool
#using Hikari connectionpool
#for options see see: https://github.com/brettwooldridge/HikariCP
spring.jpa.hibernate.connection.provider_class=org.hibernate.hikaricp.internal.HikariCPConnectionProvider
spring.datasource.hikari.maximumPoolSize=20
#remove idle conns after 5 minutes
spring.datasource.hikari.idleTimeout=300000
spring.datasource.hikari.poolName=SpringBootJPAHikariCP
#maxlife each conn is 10 min
spring.datasource.hikari.maxLifetime=600000
#wait max 10sec when creating conn
spring.datasource.hikari.connectionTimeout=10000
spring.datasource.hikari.test-on-borrow=true
spring.datasource.hikari.validation-query=SELECT 1

#DB logging
#spring.jpa.show-sql=true
#logging.level.org.hibernate.SQL=DEBUG

#flyway
spring.flyway.baseline-on-migrate=true
spring.flyway.validate-on-migrate=false
logging.level.org.flywaydb=INFO


###################
#    Resolvers    #
###################

# used to retrieve subnets of Google Public Resolver. dig TXT locations.publicdns.goog.
google.resolver.hostname=locations.publicdns.goog.
google.resolver.timeout=15

# where to retrieve subnets of OpenDNS resolver
opendns.resolver.url=https://www.opendns.com/network-map-data
opendns.resolver.timeout=15

# where to retrieve subnets of CloudFlare resolver
cloudflare.resolver.ipv4.url=https://www.cloudflare.com/ips-v4
cloudflare.resolver.ipv6.url=https://www.cloudflare.com/ips-v6
cloudflare.resolver.timeout=15

# quad9 subnets are read from embedded resource file

#################
#  AWS          #
#################

#bucket that should be used by entrada, entrada can create this
aws.bucket=sidn-entrada-test
aws.encryption=true

#Disable Cloudformation in Spring Cloud AWS
#Needs to be false when not running on EC2
cloud.aws.stack.auto=false
#Set the S3 region to use
cloud.aws.region.static=eu-west-1
#spring cloud config for aws
#see: https://cloud.spring.io/spring-cloud-static/spring-cloud-aws/2.0.0.RELEASE/multi/multi__basic_setup.html#_configuring_credentials
cloud.aws.credentials.useDefaultAwsCredentialsChain=true

#minimum size of each file part
aws.upload.multipart.mb.size=5
#number of threads to use when uploading to S3
aws.upload.parallelism=10
#storage class for parquet files uploaded to S3 bucket
aws.upload.upload.storage-class=STANDARD_IA
#storage class for pcap files uploaded to S3 bucket
aws.upload.archive.storage-class=STANDARD_IA

# Athena
athena.workgroup=primary
athena.driver.name=com.simba.athena.jdbc.Driver
athena.url=jdbc:awsathena://AwsRegion=${cloud.aws.region.static};
# where Athena will store the results of queries
athena.output.location=s3://${aws.bucket}/entrada-athena-output/
#Athena logging, will only be enabled when log4j is set to debug level
athena.log.level=4
athena.log.path=${entrada.location.work}/athena-logs/


#################
# Hadoop/Impala #
#################

#Hadoop name node hostname
hadoop.nameservice.host=
#Impala daemon hostname
impala.daemon.host=

hdfs.nameservice=hdfs://${hadoop.nameservice.host}:8020
#set the owner/group for the parquet files on HDFS, Impala have write permission
hdfs.data.owner=impala
hdfs.data.group=hive

#all Impala connection options are set using the impala.url option
#see https://www.cloudera.com/documentation/other/connectors/impala-jdbc/latest/Cloudera-JDBC-Driver-for-Impala-Install-Guide.pdf
#url examples:

#no authentication
#impala.url=jdbc:impala://${IMPALA_NODE}:21050;AuthMech=0;

#Kerberos authentication, need the following JVM args for Kerberos config and JAAS config:
# Kerberos config
#   -Djava.security.krb5.conf=/path/to/krb5.conf
# Jaas config
#   -Djava.security.auth.login.config=/path/to/jaas.conf


#Kerberos config
#username e.g. hdfs@SIDNLABS
kerberos.username=
#name of the realm to use
kerberos.realm=
#Path to the Kerberos keytab file
kerberos.keytab=

#Impala JDBC connection url
impala.url=jdbc:impala://${impala.daemon.host}:21050;AuthMech=1;KrbRealm=${kerberos.realm};KrbHostFQDN=${impala.daemon.host};KrbServiceName=impala


###############
#   MaxMind   #
###############
#names of the database files as found in the maxmind download archive
geoip.maxmind.db.name.country=GeoLite2-Country.mmdb
geoip.maxmind.db.name.asn=GeoLite2-ASN.mmdb
#url to where to download the maxmind database archives
geoip.maxmind.db.url.country=https://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz
geoip.maxmind.db.url.asn=https://geolite.maxmind.com/download/geoip/database/GeoLite2-ASN.tar.gz

# where to find (or store) the MaxMind files
geoip.maxmind.location=${entrada.location.work}/maxmind
#max number of days to use db before new db version should be downloaded
geoip.maxmind.age.max=30


###############
#   Metrics   #
###############

management.metrics.export.graphite.host=entrada-mon.sidnlabs.nl
management.metrics.export.graphite.port=2003
#management.metrics.export.tags-as-prefix=test.entrada3
management.metrics.export.graphite.protocol=Plaintext
management.metrics.export.graphite.step=1m
management.metrics.export.graphite.tags-as-prefix=prefix
management.metrics.export.graphite.prefix=entradaNew
#retention agregation, default 10sec
#http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf
management.metrics.export.graphite.retention=10

management.metrics.enable.tomcat=false
management.metrics.enable.hikaricp=false
management.metrics.enable.jdbc=false
management.metrics.enable.logback=false
management.metrics.enable.jvm=true
management.metrics.enable.process=true
management.metrics.enable.system=true



management.endpoint.metrics.enabled=true
management.endpoints.web.exposure.include=*


###############
#   LOGGING   #
###############

logging.level.nl.sidnlabs=INFO

##################
#   WEB SEREVR   #
##################

server.port=8080

