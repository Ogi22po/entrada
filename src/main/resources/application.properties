#########################
#    IN/OUT location    #
#########################

#work (always use local disk)
entrada.location.work=/Users/maartenwullink/sidn/development/tmp/entrada/work

#Local
entrada.location.input=/Users/maartenwullink/sidn/development/tmp/entrada/pcap
#entrada.location.output=/Users/maartenwullink/sidn/development/tmp/entrada/database
entrada.location.archive=/Users/maartenwullink/sidn/development/tmp/entrada/archive

#Hadoop/HDFS
#entrada.location.input=${hdfs.nameservice}/user/entrada/pcap
entrada.location.output=${hdfs.nameservice}/user/entrada/database
#entrada.location.archive=${hdfs.nameservice}/user/entrada/archive

#S3
#entrada.location.input=s3://${cloud.aws.bucket}/pcap/
#entrada.location.output=s3://${cloud.aws.bucket}/database/
#entrada.location.archive=s3://${cloud.aws.bucket}/archive


###################
#    Entrada     #
###################
#engine can be local, hadoop or aws
entrada.engine=hadoop

#name of the entrada database and tables that should be created 
entrada.database.name=entrada
entrada.database.table.dns=dns
entrada.database.table.icmp=icmp

#skip the last pcap file in the input dir, this file might still be
#written to if rsync is used.
entrada.input.file.skipfirst=false
#execute the pcap processing very minute unless already running
entrada.execution.delay=60
entrada.serverlist=ns1.dns.nl_ams1-ns1
#entrada.serverlist=
#max number of rows per parquet output file
entrada.parquet.max=3000000
#minutes before dns cache entries timeout
#required to match dns requests spanning multiple pcap files
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout=10
#minutes before tcp flows cache entries timeout
#expired tcp flows are discarded
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.tcp.flows=1
#minutes before fragmented ip cache entries timeout
#expired fragments are discarded
#time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.ip.fragmented=1
#pcap inpustream buffer in kilobytes
entrada.inputstream.buffer=64

#enable parsing of ICMP packets
entrada.icmp.enable=true

#Mode can be any of: archive, delete, none
entrada.pcap.archive.mode=archive

#check every x minutes if there are partitions that need to be compacted
entrada.parquet.compaction.interval=1
#the number of minutes to wait after the last rows have been written to the partition
#before compaction of table partition is performed.
entrada.parquet.compaction.age=1


###################
#    Database    #
###################

#Choose between PostgreSQL or H2 database

#PostgreSQL
#################################
spring.datasource.url=jdbc:postgresql://localhost:5432/entrada
spring.datasource.username=${pgsql_entrada_user}
spring.datasource.password=${pgsql_entrada_password}
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
spring.datasource.driver-class-name=org.postgresql.Driver
#################################

#H2
#################################
#spring.datasource.url=jdbc:h2:file:${entrada.location.work}/entrada.db
#spring.datasource.driver-class-name=org.h2.Driver
#spring.datasource.username=sa
#spring.datasource.password=entrada
#spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
#spring.h2.console.enabled=true
#spring.h2.console.path=/h2
#spring.jpa.hibernate.ddl-auto=none
##################################

#JPA
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
spring.jpa.generate-ddl=false
spring.jpa.properties.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.hbm2ddl.auto=none
spring.jpa.show-sql=false
spring.jpa.hibernate.format_sql=false

#Connectionpool
#using Hikari connectionpool
#for options see see: https://github.com/brettwooldridge/HikariCP
spring.jpa.hibernate.connection.provider_class=org.hibernate.hikaricp.internal.HikariCPConnectionProvider
spring.datasource.hikari.maximumPoolSize=20
#remove idle conns after 5 minutes
spring.datasource.hikari.idleTimeout=300000
spring.datasource.hikari.poolName=SpringBootJPAHikariCP
#maxlife each conn is 10 min
spring.datasource.hikari.maxLifetime=600000
#wait max 10sec when creating conn
spring.datasource.hikari.connectionTimeout=10000
spring.datasource.hikari.test-on-borrow=true
spring.datasource.hikari.validation-query=SELECT 1

#DB logging

#spring.jpa.show-sql=true
#logging.level.org.hibernate.SQL=DEBUG

#flyway
spring.flyway.baseline-on-migrate=true
spring.flyway.validate-on-migrate=false
logging.level.org.flywaydb=INFO

spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false


###################
#    Resolvers    #
###################

# used to retrieve subnets of Google Public Resolver. dig TXT locations.publicdns.goog.
google.resolver.hostname=locations.publicdns.goog.
google.resolver.timeout=15

# where to retrieve subnets of OpenDNS resolver
opendns.resolver.url=https://www.opendns.com/network-map-data
opendns.resolver.timeout=15

# where to retrieve subnets of CloudFlare resolver
cloudflare.resolver.ipv4.url=https://www.cloudflare.com/ips-v4
cloudflare.resolver.ipv6.url=https://www.cloudflare.com/ips-v6
cloudflare.resolver.timeout=15

# quad9 subnets are read from resources file (no way to fetch externally)

#################
#  AWS config   #
#################

#bucket that should be used by entrada
cloud.aws.bucket=sidn-entrada-test2

# Needs to be false when not running on EC2
cloud.aws.stack.auto=false
cloud.aws.region.static=eu-west-1
cloud.aws.credentials.useDefaultAwsCredentialsChain=true

#minimum size of each file part
cloud.aws.upload.multipart.mb.size=5
#number of thredas to use when uploading to AWS
cloud.aws.upload.parallelism=10
#storage class fo parquet files uploaded to S3 bucket
cloud.aws.upload.upload.storage-class=STANDARD_IA
cloud.aws.upload.archive.storage-class=STANDARD_IA


#################
# Hadoop/Impala #
#################

#Hadoop name node hostname
hadoop.nameservice.host=
#Impala daemon hostname
impala.daemon.host=

#all Impala connection options are set using the impala.url option
#see https://www.cloudera.com/documentation/other/connectors/impala-jdbc/latest/Cloudera-JDBC-Driver-for-Impala-Install-Guide.pdf
#url examples:

#no authentication
#impala.url=jdbc:impala://${IMPALA_NODE}:21050;AuthMech=0;

#Kerberos authentication, need the following JVM args:
# Kerberos config
#   -Djava.security.krb5.conf=/path/to/krb5.conf
# Jaas config
#   -Djava.security.auth.login.config=/path/to/jaas.conf

hdfs.nameservice=hdfs://${hadoop.nameservice.host}:8020
#set the owner/group for the parquet files on HDFS, Impala have write permission
hdfs.data.owner=impala
hdfs.data.group=hive

#Kerberos username e.g. hdfs@SIDNLABS
kerberos.username=
#Path to the Kerberos keytab file
kerberos.keytab=
impala.url=jdbc:impala://${impala.daemon.host}:21050;AuthMech=1;KrbRealm=SIDNLABS;KrbHostFQDN=${impala.daemon.host};KrbServiceName=impala

##########
# Athena #
##########

athena.workgroup=primary
athena.driver.name=com.simba.athena.jdbc.Driver
athena.url=jdbc:awsathena://AwsRegion=${cloud.aws.region.static};
# where Athena will store the results of queries
athena.output.location=s3://${cloud.aws.bucket}/entrada-athena-output/
athena.log.level=4
athena.log.path=${entrada.location.work}/athena-logs/


###############
#   MaxMind   #
###############
#names of the database files as found in the maxmind download archive
geoip.maxmind.db.name.country=GeoLite2-Country.mmdb
geoip.maxmind.db.name.asn=GeoLite2-ASN.mmdb
#url to where to download the maxmind database archives
geoip.maxmind.db.url.country=https://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz
geoip.maxmind.db.url.asn=https://geolite.maxmind.com/download/geoip/database/GeoLite2-ASN.tar.gz

# where to find (or store) the MaxMind files
geoip.maxmind.location=${entrada.location.work}/maxmind
#max number of days to use db before new db version should be downloaded
geoip.maxmind.age.max=30


###############
#   Metrics   #
###############

graphite.prefix=test.entrada
graphite.host=${metrics.host}
graphite.port=2003
#connect timeout in seconds
graphite.connect.timeout=3
#retention agregation, default 30sec
#http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf
graphite.retention=10
graphite.threshhold=200


###############
#   LOGGING   #
###############

logging.level.nl.sidnlabs=INFO

##################
#   WEB SEREVR   #
##################

server.port=8081

